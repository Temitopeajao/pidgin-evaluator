# ğŸŒ Pidgin-Evaluator  
## LLM-as-a-Judge for West African Languages

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![OpenAI](https://img.shields.io/badge/OpenAI-API-green)
![NLP](https://img.shields.io/badge/Focus-NLP-orange)
![Domain](https://img.shields.io/badge/Domain-African%20Languages-yellow)
![License](https://img.shields.io/badge/License-MIT-lightgrey)
![Status](https://img.shields.io/badge/Status-Active-success)

> Automated semantic evaluation and quality control for English â†’ Nigerian Pidgin translations using LLM judges.

---

## ğŸ“‹ Overview

**Pidgin-Evaluator** is a Python pipeline that uses the **LLM-as-a-Judge** methodology to automatically assess translation quality for **low-resource African languages**, specifically **Nigerian Pidgin**.

Traditional metrics like **BLEU**, **ROUGE**, or **chrF** fail to capture:

- Cultural nuance  
- Code-switching  
- Informal grammar  
- Conversational tone  

Instead of surface-level token matching, this tool performs **semantic reasoning-based evaluation** using a stronger LLM â€” similar to how a human reviewer would judge quality.

It effectively simulates an **RLHF-style feedback loop** for translation systems.

---

## âœ¨ Features

### ğŸ” Synthetic Translation Loop
Simulates base models (Llama, GPT-4o-mini, etc.) generating Pidgin translations.

### ğŸ§  LLM-as-a-Judge Scoring
A stronger judge model evaluates outputs on:
- Lexical accuracy
- Grammar
- Cultural authenticity
- Tone

### ğŸ“Š Likert Scale Grading
Scores each sample from **1â€“5** with structured reasoning.

### ğŸ—£ï¸ Explainable Feedback
Returns **why** a translation failed â€” not just numbers.

### ğŸ“¦ Structured JSON Outputs
Plug directly into:
- Training pipelines
- SFT filtering
- RLHF datasets
- Analytics dashboards

---

## ğŸ› ï¸ Why This Matters

### The Problem
African languages face:

- Limited datasets  
- Heavy code-switching  
- Sparse evaluation tools  
- High hallucination rates in LLMs  

### The Result
Poor training data â†’ poor models.

### The Solution
**Pidgin-Evaluator = Automatic quality gate**

âœ” Filters weak samples  
âœ” Improves fine-tuning data  
âœ” Reduces manual review  
âœ” Boosts downstream performance  

---

## ğŸ§± Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ English Sentence â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Base Translator  â”‚  (Llama / GPT-4o-mini)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Judge Model      â”‚  (GPT-4o)
                â”‚ LLM-as-a-Judge   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ JSON Output      â”‚
                â”‚ score + reason   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Installation

### 1. Clone repo

```bash
git clone https://github.com/yourusername/pidgin-evaluator.git
cd pidgin-evaluator
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set API key

Mac/Linux:
```bash
export OPENAI_API_KEY=your_key_here
```

Windows:
```bash
setx OPENAI_API_KEY "your_key_here"
```

---

## â–¶ï¸ Quick Start

```python
from evaluator import evaluate_translation

result = evaluate_translation(
    source="How are you today?",
    translation="How you dey today?"
)

print(result)
```

### Example Output

```json
{
  "score": 5,
  "reasoning": "Accurate lexical choice and natural conversational tone."
}
```

---

## ğŸ§ª Example Use Cases

- SFT dataset filtering  
- RLHF feedback simulation  
- Translation benchmarking  
- Linguistic research  
- African NLP tools  
- Model regression testing  

---

## ğŸ§° Built With

- Python
- OpenAI API (GPT-4o)
- Prompt Engineering
- JSON pipelines
- LLM Evaluation techniques
- NLP / Low-resource language research

---

## ğŸ“‚ Project Structure

```
pidgin-evaluator/
â”‚
â”œâ”€â”€ evaluator/          # Core scoring engine
â”œâ”€â”€ prompts/            # Judge prompts
â”œâ”€â”€ data/               # Sample translation sets
â”œâ”€â”€ scripts/            # Pipeline automation
â”œâ”€â”€ notebooks/          # Experiments
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”¬ Future Roadmap

- [ ] Yoruba evaluation support  
- [ ] Hausa support  
- [ ] Igbo support  
- [ ] Batch scoring  
- [ ] Web dashboard  
- [ ] HuggingFace integration  
- [ ] Human + LLM hybrid scoring  

---

## ğŸ¤ Contributing

Contributions are welcome.

Ideas:
- Better judge prompts
- More African languages
- Performance improvements
- Benchmark datasets

Open an issue or PR.

---

## ğŸ‘¤ Author

**Temitope Ajao**  
AI Engineer & LLM Specialist  

([LinkedIn](www.linkedin.com/in/temitope-ajao-4a8670302) â€¢ [Email](mailto:topekele@email.com)
)

---

## ğŸ“œ License

MIT License

---

## â­ If this project helps you
Give it a star â€” it helps visibility for African NLP research âœ¨




